# Progress Journal

## 2026-02-05: Cobalt AI Testing Framework - P0 & P1 Completion

### Project Overview

Cobalt is an AI testing framework - "Cypress for AI agents" - that provides experiment runners, evaluators, datasets, and result tracking for testing AI systems.

**Status**: P0 (MVP) and P1 (Usable) are complete ‚úÖ

---

## P0 (MVP) Implementation ‚úÖ

**Goal**: Basic functionality for running experiments and evaluations.

### Core Features Implemented

**1. Experiment Runner** (`src/core/experiment.ts`)
- ‚úÖ Parallel execution with configurable concurrency
- ‚úÖ Timeout enforcement per item
- ‚úÖ Progress tracking with callbacks
- ‚úÖ Error isolation (failed items don't block others)
- ‚úÖ Metadata collection

**2. Evaluators** (`src/core/Evaluator.ts`, `src/evaluators/`)
- ‚úÖ LLM Judge evaluator (OpenAI & Anthropic)
- ‚úÖ Function evaluator (custom JS/TS functions)
- ‚úÖ Evaluator dispatch system
- ‚úÖ Error handling (returns `{score: 0}` instead of throwing)

**3. Datasets** (`src/datasets/Dataset.ts`)
- ‚úÖ Inline dataset creation
- ‚úÖ `Dataset.fromJSON()` loader
- ‚úÖ Basic dataset operations

**4. CLI** (`src/cli/`)
- ‚úÖ `cobalt run` command with terminal reporter
- ‚úÖ `cobalt init` scaffolding command
- ‚úÖ Experiment file loading with jiti (TypeScript support)

**5. Storage** (`src/storage/`)
- ‚úÖ Save results to `.cobalt/results/*.json`
- ‚úÖ Timestamped result files

**6. Configuration** (`src/core/config.ts`)
- ‚úÖ `defineConfig()` function
- ‚úÖ Config file loading (`cobalt.config.ts`)
- ‚úÖ Environment variable support

---

## P1 (Usable) Implementation ‚úÖ

**Goal**: Production-ready features for real-world use.

### Enhanced Features

**1. Additional Evaluators**
- ‚úÖ Exact match evaluator (`src/evaluators/exact-match.ts`)
  - Case-sensitive/insensitive options
  - Field-based comparison
  - Trimming support

**2. LLM Response Cache** (`src/storage/cache.ts`)
- ‚úÖ Hash-based caching (prompt + model + content)
- ‚úÖ TTL support structure
- ‚úÖ Significant cost savings on reruns

**3. Dataset Enhancements**
- ‚úÖ `Dataset.sample(n)` - Random sampling
- ‚úÖ `Dataset.slice(start, end)` - Subset selection
- ‚úÖ `Dataset.fromCSV()` - CSV file loader
- ‚úÖ `Dataset.fromJSONL()` - JSONL file loader
- ‚úÖ Chainable transformations (map, filter)

**4. CLI Filtering**
- ‚úÖ `--filter` flag for experiment name patterns
- ‚úÖ Tag-based filtering
- ‚úÖ Flexible pattern matching

**5. Cost Tracking** (`src/utils/cost.ts`)
- ‚úÖ Token usage tracking
- ‚úÖ Cost estimation (OpenAI & Anthropic pricing)
- ‚úÖ `formatCost()` utility
- ‚úÖ Per-experiment cost reporting

**6. Statistics** (`src/utils/stats.ts`)
- ‚úÖ Aggregated score statistics
- ‚úÖ Average, min, max calculations
- ‚úÖ Percentiles (p50, p95)

---

## P1+ (Bonus Features - Implemented Early)

Some P2/P3 features were implemented ahead of schedule:

**1. History Tracking** (P2 feature)
- ‚úÖ SQLite database (`src/storage/db.ts`)
- ‚úÖ `cobalt history` command
- ‚úÖ Run metadata storage

**2. Comparison Tool** (P2 feature)
- ‚úÖ `cobalt compare <id1> <id2>` command
- ‚úÖ Side-by-side comparison UI

**3. MCP Integration** (P3 feature - partial)
- ‚úÖ MCP server (`src/mcp/server.ts`)
- ‚úÖ `cobalt mcp` command
- ‚úÖ MCP tools: `cobalt_run`, `cobalt_results`, `cobalt_compare`
- ‚ùå Missing: `cobalt_generate`, resources, prompts

**4. Dashboard API** (P4 feature - backend only)
- ‚úÖ Hono server (`src/dashboard/server.ts`)
- ‚úÖ API endpoints: `/api/runs`, `/api/runs/:id`, `/api/compare`, `/api/trends`
- ‚úÖ `cobalt serve` command
- ‚ùå Missing: React frontend UI

**5. Clean Command** (Utility)
- ‚úÖ `cobalt clean` command
- ‚úÖ Cache cleanup
- ‚úÖ Results cleanup

---

## 2026-02-05: Test Suite Implementation (Phase 9) ‚úÖ

### Comprehensive Test Suite

**Goal**: Achieve strong test coverage for all P0/P1 core features.

**Result**: 138 tests created across 8 test files, all passing ‚úÖ

### Test Infrastructure

**Created Files:**
1. `tests/helpers/mocks.ts` - Mock data and factory functions
2. `tests/helpers/fixtures.ts` - Sample datasets and test data

**Mocking Strategy:**
- LLM APIs: Mock `openai` and `@anthropic-ai/sdk` modules
- File system: Mock `node:fs` for dataset loading
- No real API calls in tests (fast, free, deterministic)

---

### Test Coverage Breakdown

**8 Test Files Created:**

1. **`tests/unit/Dataset.test.ts`** - 36 tests
   - Constructor and inline dataset creation
   - `fromJSON()` - array and object formats
   - `fromJSONL()` - line-delimited JSON parsing
   - `fromCSV()` - CSV with headers and quoted values
   - Transformations: `map()`, `filter()`, `sample()`, `slice()`
   - Edge cases and error handling

2. **`tests/unit/Evaluator.test.ts`** - 23 tests
   - Constructor defaults and type inference
   - Dispatch to correct evaluator implementation
   - LLM judge, function, exact-match dispatch
   - Error handling (similarity evaluator stub)
   - Unknown evaluator type handling
   - Multiple evaluators running independently

3. **`tests/unit/evaluators/llm-judge.test.ts`** - 13 tests
   - OpenAI API integration (mocked)
   - Anthropic API integration (mocked)
   - Template variable replacement
   - JSON response parsing
   - Markdown code block stripping
   - Error handling for API failures
   - Missing API key validation

4. **`tests/unit/evaluators/function.test.ts`** - 10 tests
   - Custom function execution
   - Score validation (0-1 range)
   - Async function support
   - Error handling for invalid scores
   - Default reason when not provided

5. **`tests/unit/evaluators/exact-match.test.ts`** - 16 tests
   - Exact string matching
   - Case-sensitive vs insensitive
   - Trimming whitespace
   - Field-based comparison
   - Missing field handling
   - Edge cases

6. **`tests/unit/utils/cost.test.ts`** - 17 tests
   - Cost estimation for OpenAI models
   - Cost estimation for Anthropic models
   - Unknown model fallback
   - Token counting (input + output)
   - Cost formatting

7. **`tests/unit/utils/stats.test.ts`** - 12 tests
   - Average, min, max calculations
   - Percentile calculations (p50, p95)
   - Interpolated median
   - Empty array handling
   - Single value edge case

8. **`tests/unit/utils/template.test.ts`** - 17 tests
   - Variable replacement (`{{variable}}`)
   - Multiple variables in template
   - Missing variables handling
   - Nested properties (not supported, verified)
   - Edge cases

---

### Test Fixes Applied

During implementation, several mismatches were discovered and fixed:

**1. Cost estimation parameter order**
- ‚ùå Expected: `estimateCost(model, tokens)`
- ‚úÖ Actual: `estimateCost(tokens, model)`
- Fix: Updated tests to match actual signature

**2. Stats p50 calculation**
- ‚ùå Expected: Lower value for 2-item array
- ‚úÖ Actual: Interpolated median
- Fix: Updated expectations to match interpolation logic

**3. Template engine limitations**
- ‚ùå Expected: Support for `{{metadata.model}}`
- ‚úÖ Actual: Only top-level variables
- Fix: Updated tests to verify nested properties stay unreplaced

**4. Exact match field name**
- ‚ùå Used: `caseInsensitive: true`
- ‚úÖ Actual: `caseSensitive: false`
- Fix: Updated config field name

**5. Function evaluator validation**
- ‚ùå Expected: Score clamping
- ‚úÖ Actual: Throws error for invalid scores
- Fix: Changed tests to expect rejection

**6. LLM judge message structure**
- ‚ùå Expected: User prompt in `messages[0]`
- ‚úÖ Actual: System in `[0]`, user in `[1]` (OpenAI)
- Fix: Updated test assertions

**7. Evaluator error handling**
- ‚ùå Expected: Errors throw/reject
- ‚úÖ Actual: Returns `{score: 0, reason: "error..."}`
- Fix: Changed tests to expect resolved error results

---

### Coverage Metrics

**Overall Project Coverage: 17.2%**

**Tested Modules Coverage:**
- ‚úÖ Evaluator class: 100% statement coverage
- ‚úÖ Dataset class: ~90% coverage (all methods tested)
- ‚úÖ LLM judge evaluator: 80-95% coverage
- ‚úÖ Function evaluator: 80-95% coverage
- ‚úÖ Exact match evaluator: 80-95% coverage
- ‚úÖ Utility functions (cost, stats, template): 95%+ coverage

**Untested Modules (P2/P3/P4 features):**
- ‚è≠Ô∏è CLI commands (run, init, serve, history, compare, clean)
- ‚è≠Ô∏è Config file loading (jiti integration)
- ‚è≠Ô∏è Experiment runner (parallel execution, timeouts)
- ‚è≠Ô∏è Dashboard server & API endpoints
- ‚è≠Ô∏è MCP server & tools
- ‚è≠Ô∏è Storage layer (results, cache, SQLite)
- ‚è≠Ô∏è End-to-end integration tests

---

### Test Categories Covered

- ‚úÖ Data loading (JSON, JSONL, CSV)
- ‚úÖ Data transformations (map, filter, sample, slice)
- ‚úÖ Evaluator dispatch logic
- ‚úÖ LLM judge (OpenAI & Anthropic mocking)
- ‚úÖ Function evaluators with validation
- ‚úÖ Exact string matching
- ‚úÖ Template variable replacement
- ‚úÖ Cost estimation for multiple models
- ‚úÖ Statistical calculations (avg, min, max, percentiles)
- ‚úÖ Error handling and edge cases

---

### Future Test Work

**Integration Tests (P2+):**
1. Full experiment execution flow (end-to-end)
2. CLI command tests with mocked file system
3. Config loading tests
4. Storage layer tests (SQLite, file I/O)
5. Dashboard API endpoint tests
6. MCP server tool tests

**Current Status**: Core P0/P1 functionality is well-tested. Integration and higher-level feature tests are deferred to P2+.

---

## 2026-02-05: Documentation Overhaul ‚úÖ

### Documentation Fixes

**Problem**: All documentation described a CQRS web app instead of the actual AI testing framework.

**Solution**: Complete rewrite of all documentation files to match reality.

**Files Updated:**
1. ‚úÖ Root `README.md` - User-facing documentation for AI testing framework
2. ‚úÖ `CLAUDE.md` - Development guide for CLI tool development
3. ‚úÖ `.memory/analysis.md` - Actual project structure and architecture
4. ‚úÖ `.memory/decisions.md` - Technical decisions for AI testing framework
5. ‚úÖ `.memory/documentation.md` - Complete API reference
6. ‚úÖ `.memory/progress.md` - This file (cleaned up phases 1-8)
7. ‚úÖ `packages/cobalt/README.md` - Package-specific documentation (to be created)

**Impact**: Documentation now accurately reflects the AI testing framework that was actually built.

---

## Statistics (Current)

**Project Metrics:**
- **Packages**: 1 (cobalt - single package structure)
- **Lines of Code**: ~3,300 (src/)
- **Test Files**: 8
- **Test Cases**: 138
- **Test Coverage**: 17.2% overall (80-100% for tested modules)
- **CLI Commands**: 7 (run, init, history, compare, serve, clean, mcp)
- **Evaluator Types**: 3 implemented (llm-judge, function, exact-match)
- **Dataset Formats**: 3 (JSON, JSONL, CSV)

**Phase Completion:**
- ‚úÖ P0 (MVP): 100% complete
- ‚úÖ P1 (Usable): 100% complete
- ‚úÖ **P2 (Powerful): 100% complete** (4/4 features) ‚Üê NEW!
- ‚ö†Ô∏è P3 (Connected): ~10% complete (1/8 features, MCP 40% done)
- ‚ö†Ô∏è P4 (Dashboard): 25% complete (backend only, no UI)

---

## 2026-02-06: P2 (Powerful) Features Completed ‚úÖ

### Implementation Summary

**Goal**: Complete remaining P2 features for production-ready AI agent testing.

**Status**: P2 100% complete ‚úÖ

### Features Implemented

#### 1. Similarity Evaluator with Embeddings ‚úÖ

**Implementation** (`src/evaluators/similarity.ts`):
- OpenAI embeddings API integration (text-embedding-3-small)
- Cosine similarity calculation between vectors
- Threshold mode: binary scoring (1 if similarity ‚â• threshold, else 0)
- Raw similarity mode: continuous scoring (0-1)
- Field-based comparison from dataset items
- Error handling for missing API keys and empty text

**Files Created/Modified:**
- ‚úÖ NEW: `src/evaluators/similarity.ts` (122 lines)
- ‚úÖ Modified: `src/core/Evaluator.ts` (added similarity case)
- ‚úÖ NEW: `tests/unit/evaluators/similarity.test.ts` (14 tests)
- ‚úÖ Modified: `tests/helpers/mocks.ts` (added embedding mock)

**Test Coverage**: 14 tests covering:
- High/low similarity detection
- Threshold vs raw similarity modes
- Field extraction
- Edge cases (empty text, missing API key, API errors)
- Zero magnitude vectors

**Key Design Decisions:**
- OpenAI-only for v1 (can add more providers in P3)
- Uses cost-effective text-embedding-3-small by default
- Cosine similarity normalized to [0, 1] range

#### 2. Multiple Runs Support ‚úÖ

**Implementation** (across multiple files):
- Sequential runs per item, parallel across items (decision #3)
- Store all individual run results (decision #2)
- Comprehensive statistics: mean, stddev, min, max, p50, p95 (decision #5)
- Hybrid progress reporting (decision #4)

**Files Created/Modified:**
- ‚úÖ Modified: `src/types/index.ts` - Extended ItemResult with runs[] and aggregated
- ‚úÖ NEW: Added RunAggregation and SingleRun types
- ‚úÖ Modified: `src/utils/stats.ts` - Added standardDeviation() and calculateRunStats()
- ‚úÖ Modified: `src/core/runner.ts` - Complete rewrite for multiple runs support
- ‚úÖ Modified: `src/core/experiment.ts` - Updated progress and summary calculation
- ‚úÖ Extended: `tests/unit/utils/stats.test.ts` (added 15 new tests for stddev and run stats)

**Test Coverage**: 27 tests for stats (12 existing + 15 new):
- Standard deviation calculation
- Run aggregation statistics
- High/low variance handling
- Empty arrays and edge cases

**Key Features:**
- Backward compatible: `runs=1` behaves exactly as before
- Progress shows: "Progress: 15/50 completed | Item 3/10 (run 2/5)"
- Result structure includes both flat fields (compat) and runs[] array
- Aggregated statistics per evaluator across all runs
- Summary collects scores from all runs (10 items √ó 5 runs = 50 scores)

#### 3. Integration ‚úÖ

**Verification:**
- ‚úÖ All 167 tests passing (increased from 152)
- ‚úÖ Build succeeds with no errors
- ‚úÖ TypeScript strict mode compliance
- ‚úÖ Both features work together (similarity evaluator + multiple runs)

---

## Statistics (Updated)

**Project Metrics:**
- **Lines of Code**: ~3,800 (src/) [+500 from P2]
- **Test Files**: 9 [+1 similarity test]
- **Test Cases**: 167 [+15 from P2]
- **Test Coverage**: ~20% overall (85-100% for tested modules)
- **CLI Commands**: 7 (unchanged)
- **Evaluator Types**: 4 implemented (llm-judge, function, exact-match, **similarity** ‚Üê NEW)
- **Dataset Formats**: 3 (JSON, JSONL, CSV)

**Phase Completion:**
- ‚úÖ P0 (MVP): 100% complete
- ‚úÖ P1 (Usable): 100% complete
- ‚úÖ **P2 (Powerful): 100% complete** ‚Üê NEW!
- ‚ö†Ô∏è P3 (Connected): ~10% complete
- ‚ö†Ô∏è P4 (Dashboard): 25% complete

---

## Next Steps

### Immediate (Documentation Completion)
- ‚úÖ Root README.md updated
- ‚úÖ CLAUDE.md updated
- ‚úÖ .memory/ files updated
- üîÑ packages/cobalt/README.md (in progress)

### P2 (Powerful) Features
1. **Similarity evaluator** - Embeddings-based evaluation
   - OpenAI text-embedding-3-small/large
   - Cohere embeddings
   - Semantic similarity scoring

2. **Multiple runs aggregation** - Run same item N times
   - Statistical aggregation (mean, stddev)
   - Confidence intervals
   - Non-determinism handling

3. **Complete compare/history** (already done early)

### P3 (Connected) Features
1. **Complete MCP implementation**
   - `cobalt_generate` tool (auto-generate experiments)
   - MCP resources
   - MCP prompts

2. **CI mode** - Thresholds and exit codes
   - `--ci` flag
   - Threshold-based pass/fail
   - Minimal output for CI logs

3. **Remote datasets** - bdataset, HTTP URLs, S3

4. **RAGAS integration** - Built-in RAGAS-style evaluators

### P4 (Dashboard) Features
1. **React UI** - Build dashboard frontend
   - Runs list page
   - Run detail page
   - Compare page
   - Trends visualization

2. **Real-time updates** - WebSocket support

3. **Export** - CSV and Markdown export

---

## Lessons Learned

1. **Test early** - Having 138 tests provides confidence in refactoring
2. **Documentation matters** - Outdated docs are worse than no docs
3. **Mocking strategy** - Mocking LLM APIs enables fast, free testing
4. **Immutable transformations** - Dataset transformations being immutable prevents bugs
5. **Error handling** - Returning `{score: 0}` instead of throwing keeps experiments running
6. **Cache is valuable** - LLM response caching saves significant costs on reruns

---

## Acknowledgments

This project was developed collaboratively with Claude Sonnet 4.5, following the collaborative decision-making philosophy outlined in CLAUDE.md.
